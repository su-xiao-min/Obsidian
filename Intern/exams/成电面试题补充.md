## 问题

你解释一下Raft算法
扩展一下服务降级和熔断，
另外，
我说Redis有其他的进程，
还是面试官提醒我的，
因为Redis需要对于数据进行持久化，
比如说RDB就需要另开一个进程，

详细介绍一下Cache Aside 的双删策略。
消息队列最么保证最终一致性的，也请解释一下。

之后，Redis相关问题
哨兵如何发现彼此？
如何选择领导哨兵？
如何应对脑裂？
我都不知道。
因为我看的面经里面，这一块是收费的

### Redis相关漏洞

Redis Sentinel：什么是 Sentinel？ 有什么用？Sentinel 如何检测节点是否下线？主观下线与客观下线的区别？Sentinel 是如何实现故障转移的？为什么建议部署多个 sentinel 节点（哨兵集群）？Sentinel 如何选择出新的 master（选举机制）？如何从 Sentinel 集群中选择出 Leader？Sentinel 可以防止脑裂吗？Redis Cluster：为什么需要 Redis Cluster？解决了什么问题？有什么优势？Redis Cluster 是如何分片的？为什么 Redis Cluster 的哈希槽是 16384 个？如何确定给定 key 的应该分布到哪个哈希槽中？Redis Cluster 支持重新分配哈希槽吗？Redis Cluster 扩容缩容期间可以提供服务吗？Redis Cluster 中的节点是怎么进行通信的？

这一块我都不知道。

MySQL的主键应该是整体的数据的排序，
不指定，
它也会有一个隐藏的主键，
只不过排序可能不是按照我们期待的方式。

### 索引下推，

比如说 ABC的联合索引，
如果提供AB，那么也能够使用这个联合索引

### 覆盖索引

如果想要查询的数据就在ABC当中，
就不需要再查询了。

### 慢SQL

使用explain看一下执行的方式，
看一下索引是否有被使用到，

JVM和并发先放弃

### 系统设计

是的，这就是我在思考的地方
我也不知道我们的兜底机制如果都失效了
还有什么可以依赖
但是，
我们貌似只需要最终一致性
我不太理解你提供的策略。
详细解释一下三个策略

不一致，
怎么解决呢？

热点key我也不会。
可以把热点key分散一下，把它使用分片的技术，
拆成多个，
但是细节我就不太知道了。

# DeepSeek

## **Raft算法解释**

Raft是一种**分布式一致性算法**，用于管理复制状态机的一致性。它将一致性分解为三个相对独立的子问题：

1. **领导者选举（Leader Election）**
2. **日志复制（Log Replication）**
3. **安全性（Safety）**

### **核心机制：**

- **角色**：每个节点有三种状态：
  - **领导者（Leader）**：处理所有客户端请求，管理日志复制
  - **追随者（Follower）**：被动响应领导者的心跳和日志复制请求
  - **候选人（Candidate）**：在选举期间临时状态

- **任期（Term）**：逻辑时钟，每个任期最多一个领导者。任期号单调递增。

- **领导者选举流程**：
  1. 追随者在一定时间（选举超时，随机150-300ms）内没收到领导者心跳，就转变为候选人。
  2. 候选人递增当前任期，给自己投票，并向其他节点发送**RequestVote RPC**请求投票。
  3. 其他节点收到请求后，在**一个任期内只能投一票**（先到先得），且只会投票给**日志至少和自己一样新**的候选人。
  4. 候选人如果获得**超过半数**的投票，就当选为领导者。
  5. 领导者立即向所有节点发送**心跳（AppendEntries RPC）**来宣告权威。

- **日志复制**：
  1. 客户端向领导者发送命令。
  2. 领导者将命令作为新条目追加到自己的日志中。
  3. 领导者并行地向所有追随者发送**AppendEntries RPC**复制该条目。
  4. 当**超过半数**的追随者确认复制后，领导者**提交**该条目（应用到状态机），并通知追随者提交。
  5. 此时可以响应客户端。

- **安全性保证**：
  - **选举限制**：确保只有包含所有已提交日志条目的节点才能成为领导者。
  - **提交规则**：领导者只能提交当前任期的日志条目，不能直接提交之前任期的条目。

**简单理解**：Raft通过**强领导者模型**简化了一致性，领导者决定一切。它用**随机超时**避免选举冲突，用**多数派原则**保证安全性。

---

## **服务降级与熔断扩展**

### **服务降级（Service Degradation）**

**定义**：当系统资源（如CPU、内存、数据库连接）紧张时，主动**暂时关闭或简化某些非核心功能**，以保证核心功能的可用性和稳定性。

**实现方式**：

1. **功能降级**：
   - 关闭次要功能（如商品详情页关闭“猜你喜欢”）。
   - 返回简化数据（如只返回核心字段，不返回详情）。
   - 返回静态/缓存数据（如将动态推荐改为静态热榜）。

2. **读写降级**：
   - 写降级：将同步写改为异步写（如点赞异步化）。
   - 读降级：从主库降级到从库，甚至到缓存。

3. **自动降级策略**：
   - **超时降级**：调用下游服务超时后，返回默认值。
   - **失败次数降级**：连续失败N次后，暂时屏蔽该服务。
   - **限流降级**：达到QPS阈值后，排队或拒绝。

### **熔断（Circuit Breaker）**

**定义**：当下游服务**失败率超过阈值**时，**自动切断调用**，直接返回失败（或降级结果），防止故障蔓延和资源耗尽。

**状态机（三种状态）**：

1. **闭合（Closed）**：正常调用下游。
2. **开启（Open）**：直接失败，不调用下游。
3. **半开（Half-Open）**：允许少量试探请求，成功则转闭合，失败则保持开启。

**熔断器关键参数**：

- **失败率阈值**（如50%）。
- **滑动窗口大小**（最近10秒）。
- **熔断后恢复时间**（如5秒后进入半开状态）。

**类比**：电路保险丝。下游服务故障就像短路，熔断器迅速断开，保护上游系统不被拖垮。

**常用工具**：Hystrix（Netflix）、Resilience4j、Sentinel（阿里）。

---

## **Redis持久化与多进程**

你的理解基本正确：

- **RDB持久化**：通过 `SAVE`（阻塞主进程）或 `BGSAVE`（`Background Save`，**fork子进程**）生成数据快照。子进程负责将内存数据写入RDB文件，父进程继续服务。
- **AOF持久化**：主进程将写命令追加到AOF缓冲区，根据策略（always/everysec/no）由**主进程或后台线程**刷盘。AOF重写（`BGREWRITEAOF`）会**fork子进程**来生成新的AOF文件。

**关键点**：fork操作是**写时复制（Copy-On-Write）**的，子进程与父进程共享内存页，只有当父进程修改数据时，才会复制被修改的页。所以fork本身很快，但数据修改频繁时，会复制大量内存页，可能导致**内存膨胀**和**延迟抖动**。

---

## **Cache Aside模式与延迟双删策略**

### **问题场景**

并发下数据不一致：

1. 线程A更新数据库（将值从1改为2）。
2. 线程B读缓存（缓存旧值1），同时线程A删除缓存。
3. 线程B将读到的旧值1写回缓存。
4. 结果：数据库是2，缓存是1，不一致。

### **延迟双删策略**

在“更新数据库后，删除缓存”的基础上，**增加一次延迟删除**。

**步骤**：

1. **第一次删除**：更新数据库前，先删除缓存（可选，清除更旧的脏数据）。
2. **更新数据库**。
3. **第二次删除**：更新数据库后，**立即删除缓存**。
4. **第三次删除（延迟删除）**：**等待一小段时间（如500ms）后，再次删除缓存**。

**为什么需要延迟？**

- 为了确保在“更新数据库”和“第一次删除缓存”之间，所有可能读到旧数据并回填缓存的操作都已完成。
- 延迟时间需要**大于一次“读数据库+写缓存”的平均耗时**。

**注意事项**：

- 延迟删除可以通过**消息队列延迟消息**或**定时任务**实现。
- 该策略**不能100%保证强一致**，但能极大降低不一致窗口。
- 在读写分离架构下，还需要考虑**主从延迟**，延迟时间应大于主从延迟。

---

## **消息队列保证最终一致性**

### **核心原理**

通过 **“至少一次投递” + “消费者幂等性”** 保证。

### **具体保障机制**

1. **生产者端可靠投递**：
   - **本地事务+消息表**：在业务事务中，将要发送的消息插入本地消息表，事务提交后，后台任务扫描消息表并发送到MQ。发送成功后标记消息为“已发送”。
   - **事务消息**（如RocketMQ）：两阶段提交。先发送“预备消息”，本地事务执行成功后再确认提交，MQ才会投递给消费者。

2. **消息队列端持久化与复制**：
   - 消息写入磁盘，并通过多副本机制（如Kafka的ISR）保证不丢失。

3. **消费者端可靠消费与幂等**：
   - **消费者确认（ACK）机制**：只有消费者处理成功并手动ACK后，MQ才删除消息。如果处理失败或超时，MQ会重投（可能投递给其他消费者）。
   - **幂等性设计**：由于网络重试，同一条消息可能被消费多次。消费者必须实现**幂等**：
     - **唯一键法**：利用数据库唯一约束（如业务唯一ID）。
     - **状态机法**：只有当前状态符合时才处理，并更新状态。
     - **去重表法**：消费前先插入去重表（如Redis set），成功插入才处理。

4. **补偿与对账**（兜底）：
   - 定期扫描业务数据和消息消费记录，发现不一致时进行修复（如重新发送消息或人工干预）。

**完整流程示例（订单支付成功更新库存）**：

1. 支付服务本地事务：更新订单状态为“已支付” + 插入消息表（主题：`order_paid`）。
2. 消息发送服务读取消息表，发送到MQ。
3. 库存服务消费 `order_paid` 消息，检查去重表（已消费则跳过），扣减库存，插入去重记录，然后ACK。
4. 若库存服务崩溃未ACK，MQ重投，因去重表已存在，幂等跳过。

---

## **Redis Sentinel 详细解答（你“不知道”的部分）**

### **1. 哨兵如何发现彼此？**

- **自动发现**：哨兵通过订阅Redis主节点的 `__sentinel__:hello` 频道，定期向该频道发布自己的信息（IP、端口、runid等），同时监听该频道以获取其他哨兵的信息。
- **手动配置**：也可以在哨兵配置文件中直接指定其他哨兵的地址。

### **2. 如何选举领导哨兵？**

使用 **Raft算法**（简化版）：

1. 当某个哨兵判定主节点客观下线后，它向其他哨兵发送命令，请求成为领导者。
2. 其他哨兵**在同一任期内，对第一个收到的请求投赞成票**。
3. 如果该哨兵获得**超过半数**的赞成票，则成为领导者。
4. 如果没有选出，等待随机时间后开始下一轮选举。

### **3. 如何应对脑裂（Network Partition）？**

脑裂指网络分区导致集群中出现两个或多个主节点。哨兵机制通过以下方式**缓解**（无法完全避免）：

- **客观下线需要多数哨兵同意**：只有多数哨兵都认为主节点下线，才会触发故障转移。这要求故障转移发起方必须能与多数哨兵通信。
- **旧主节点降级**：故障转移后，新主节点产生。当旧主节点网络恢复后，哨兵会强制其执行 `SLAVEOF` 命令，使其成为新主节点的从节点。
- **配置纪元（Configuration Epoch）**：每次故障转移都有一个递增的纪元号，携带更高纪元号的配置会覆盖旧配置。

**但脑裂期间的数据丢失风险依然存在**：在分区期间，客户端可能仍向旧主写入数据。当旧主恢复成从节点后，这些数据会被清空，从新主同步，导致写入丢失。

- **缓解**：可设置 `min-slaves-to-write`（主节点至少需要N个从节点才能写），但会降低可用性。

---

## **系统设计：兜底机制与热点Key**

### **三个兜底策略详解**

1. **人工干预（兜底的最后防线）**
   - **监控报警**：系统监控到不一致（如Redis与DB计数差异持续超过阈值）时，触发报警。
   - **人工操作台**：提供后台界面，允许运维人员**手动触发数据修复**（如重置缓存、触发重新同步）。
   - **这是必须有的**，因为任何自动系统都可能失效。

2. **定期对账（T+1离线修复）**
   - **离线作业**：每天凌晨低峰期，运行MapReduce/Spark作业，扫描全量数据，对比Redis与DB。
   - **生成修复脚本**：发现差异后，自动生成修复任务（如将DB最新数据刷到Redis）。
   - **作用**：解决**累积性**的微小不一致，保证长期数据最终一致。

3. **业务容忍与补偿（实时柔性处理）**
   - **最终一致性时间窗口**：如点赞数允许在1分钟内最终一致。前端展示可略带模糊（如“10万+”）。
   - **补偿任务**：消息队列消费失败后，进入**死信队列（DLQ）**。独立的补偿服务消费DLQ，进行重试或转人工。
   - **版本号/时间戳**：每次更新携带版本号，缓存数据过期或版本过低时主动失效。

### **不一致的具体解决流程**

当对账发现 `video:123` 点赞数在Redis是10000，在DB是10005：

1. **判断谁可信**：通常**DB是权威数据源**（因为持久化、有事务）。
2. **修复动作**：直接向Redis发送 `SET count:like:123 10005`。
3. **修复时机**：立即修复（影响小）或低峰期批量修复。

### **热点Key分片方案（详细）**

**问题**：一个Key（如 `count:like:video:8888`）每秒百万请求，单Redis分片扛不住。

**解决方案：本地缓存+分片Key**

1. **识别与通知**：监控系统识别热Key，通知所有应用服务器。
2. **本地缓存**：应用服务器将该热Key的值缓存在本地内存（如Guava Cache，TTL设500ms）。
3. **分片Key（解决单个Redis分片瓶颈）**：
   - **写操作**：不再写单个Key，而是写多个分片Key。
     - 例如：原Key `count:like:video:8888` 拆分为10个：
       `count:like:video:8888:0` 到 `count:like:video:8888:9`
     - 写时，根据用户ID哈希或随机选择其中一个分片Key进行 `INCR`。
   - **读操作（聚合）**：
     - 读取所有10个分片Key的值，在内存中求和。
     - 由于是热Key，可以将聚合结果也放在本地缓存，定期（如100ms）更新。
4. **数据迁移**：热Key降温后，系统需将分片数据合并回原Key，并撤销分片逻辑。

**技术要点**：

- 分片数需要提前规划，动态增减较复杂。
- 需更新客户端SDK，对业务透明是挑战。
- 更适合**读多写少**的热Key（如计数）。对于**写热Key**，分片写入能有效分散压力。

---

## **给你的最后忠告**

你现在暴露的最大问题是：**知识碎片化且存在大量“想当然”的误解**。你依赖的“面经”质量堪忧，甚至包含收费陷阱。

**立即行动建议：**

1. **放弃所有零碎面经**，系统学习以下资源（中文即可）：
   - **Redis**：《Redis设计与实现》或Redis官方文档。
   - **MySQL**：《高性能MySQL》第4章索引、第5章架构。
   - **分布式**：《数据密集型应用系统设计》第8、9章。
   - **JVM**：《深入理解Java虚拟机》第3章垃圾收集器。
2. **动手实验**：
   - 本地搭建Redis Sentinel/Cluster，模拟主节点宕机、网络分区。
   - 写代码模拟缓存雪崩、穿透、击穿场景及解决方案。
3. **建立知识框架**：
   - 对每个技术，画出它的**架构图**（如哨兵集群交互图）。
   - 整理**面试问答清单**，用自己的语言重新组织答案，并找人模拟面试。

**时间不多了**。字节跳动面试官会深入追问直到你答不出来为止，你的目标是**把核心知识挖深**，而不是广撒网。优先保证Redis、MySQL、系统设计这三块能达到“深入”级别。

祝你成功。

更适合读多写少的热Key（如计数）。对于写热Key，分片写入能有效分散压力。

你在表达什么，
我看不懂，
分片解决的到底是什么问题？
一个hotkey 和把它分散成一个没有那么hot 的key到底有什么区别，
写的次数不变，
读还变慢了啊。
读多写少不适合这种方法吧，
因为读的成本变大了。
