# 提问

知识点：云服务器

背景：我正在学习基于云服务器开发的项目。

我已知：

开发大模型相关的应用或者部署，需要使用GPU，像是GPU这种东西，本地的电脑很难有，所以往往需要使用云服务器进行。

云服务器很多时候存在两种启动方式，无卡模式启动，这时候仅仅提供CPU资源，或者一般启动方式，附带GPU。

对于服务商来说，CPU资源似乎相当廉价。

比较麻烦的一个问题是，如何进行大量数据的传递。

一种方法是，采用无卡模式，传递数据之后，再打开GPU。

另外一种方式，采用类似于OSS之类的服务，先把数据传递到服务器端的数据存储中心，之后一般启动，但是从数据存储中心传递到购买的云服务器上面就会非常快。





我不懂：

问题一

云服务器的网络很差，无法下载HuggingFace的模型，甚至无法git clone，

为什么会出现这样的情况？

我目前的处理方式是，本地下载好，再上传到云端，是否存在其他的解决办法？

问题二

在服务器提供商那里选择一张显卡，对于服务商来说究竟意味着什么？

为什么选择显卡之后还需要选择实例镜像？为什么，某些框架版本下的实例镜像无法选择，就比如说，我无法使用4090-24G显卡，同时搭配PyTorch 2.9.1 和CUDA12.8.93，换言之，这时候我只能选择更低版本的PyTorch。

我的部分想法是，服务提供商为我设置了一个镜像，使用容器技术，同时，为这个镜像提供了GPU的接口，这个镜像是我自己拥有的。

在我不使用镜像的时候，服务提供商会为我保存一段时间数据，当清空数据之后，我再打开镜像，其实对于服务商来说，就好像我重新开通了一个实例。

那么进一步追问，是否所有的云服务器都是基于镜像的，或者什么情况下，它会基于虚拟机，两种实现存在什么差异？

问题三

在这种情况下，容器怎么暴露端口？

我选择的恒源云其实提供了方法，监听0.0.0.0地址，暴露8080端口，之后就可以通过他提供的一个IP地址和端口进行访问，所以，其实他提供了端口映射功能。

请你按顺序做：

1) 用最少概念解释 云服务器（不超过 120 字）

2) 指出我理解中存在的问题

3) 回答我的问题

4) 给 多个类比，但指出类比哪里会误导

5) 问我 5 个递进问题来检查理解（先别给答案）

6) 我回答后，你再指出我的漏洞并纠正

我这边网络不好应该不影响它回复我，虽然主打一个流式输出。



### 1) 云服务器解释
云服务器是服务商通过虚拟化技术提供的远程计算资源，用户按需租用CPU、内存、存储和网络，可灵活配置操作系统和软件，通过网络访问和管理，支持弹性伸缩和按使用付费。

### 2) 指出理解中存在的问题
您的理解基本合理，但有几个点可以澄清：
- 云服务器网络差可能不单纯是带宽问题，还涉及数据中心位置、国际出口限制、防火墙策略或服务商的网络优化不足。
- 数据传递中，从OSS到云服务器快，是因它们通常在同一内网或区域网络，降低了延迟和成本，但并非所有服务商都如此。
- 镜像与虚拟机/容器的关系：云服务器常基于虚拟机（完整操作系统虚拟化）或容器（轻量级应用隔离），镜像是它们的预配置模板。您提到“容器技术”，但服务商可能用虚拟机镜像，容器则用于应用层。
- 端口暴露中，恒源云的端口映射可能涉及负载均衡器或NAT网关，而非直接容器暴露。

### 3) 回答您的问题
#### 问题一：云服务器网络差的原因及解决办法
- **原因**：可能包括：1) 数据中心位于海外或网络路由不佳，导致访问HuggingFace等国际站点慢；2) 服务商为控制成本限制出口带宽；3) 防火墙或安全组规则阻挡下载；4) DNS解析问题。
- **其他解决办法**：1) 使用代理或VPN加速国际访问；2) 选择支持高速网络或优化国际链路的数据中心区域；3) 利用服务商提供的镜像市场或缓存服务（如预置模型的镜像）；4) 使用开源工具（如`huggingface-cli`的镜像源）或CDN。

#### 问题二：显卡选择对服务商的意义及镜像限制
- **对服务商的意义**：选择显卡意味着服务商需从物理GPU池中分配专属资源（如通过PCIe透传或虚拟化），涉及硬件调度、功耗管理和成本（GPU更昂贵）。
- **为什么选择镜像**：镜像是预装操作系统、驱动和软件的模板，确保环境一致性。不同镜像版本对应不同的驱动/CUDA/框架兼容性，简化用户部署。
- **框架版本无法选择的原因**：4090显卡需特定驱动和CUDA版本支持。若PyTorch 2.9.1 + CUDA 12.8.93未在镜像中提供，可能是服务商尚未测试或打包该组合，因框架与驱动兼容性需验证。您可尝试自定义安装，但可能影响稳定性。
- **云服务器基于镜像还是虚拟机**：大部分云服务器基于虚拟机（如KVM），镜像为虚拟机模板；容器（如Docker）则用于应用部署，常运行在虚拟机内。服务商保存数据是通过持久化存储（如云硬盘），清空数据后重启实例类似虚拟机重置，但硬件资源可能重新分配。

#### 问题三：容器如何暴露端口
- **机制**：容器内应用监听0.0.0.0（所有网络接口），然后通过主机端口映射将容器端口绑定到主机IP。在云环境中，服务商添加一层网络转发：将公网IP的特定端口映射到您实例的私有IP和容器端口。
- **恒源云示例**：您监听0.0.0.0:8080，服务商通过NAT或负载均衡器将公网IP:端口转发到您的实例，实现外部访问。这隐藏了底层网络细节，但依赖服务商网络配置。

### 4) 多个类比及误导处
- **类比1：云服务器像租用酒店房间**  
  误导：酒店房间固定且独立，而云服务器可随时调整配置（如扩CPU），且资源共享。
- **类比2：镜像像预制菜**  
  误导：预制菜直接食用，但镜像需启动并运行，且允许用户自定义（如安装软件）。
- **类比3：GPU选择像加油站选油号**  
  误导：油号标准统一，但GPU驱动/框架组合多样，兼容性更复杂。
- **类比4：端口映射像公司总机转分机**  
  误导：总机转接是人工的，而端口映射是自动网络路由，可能涉及多层转发。
- **类比5：容器像轻量化虚拟机**  
  误导：容器共享主机内核，启动快；虚拟机模拟完整硬件，更隔离但开销大。

### 5) 问5个递进问题来检查理解
1. 云服务器中，虚拟机和容器在资源隔离和性能开销上有何主要区别？
2. 为什么服务商提供不同CUDA版本的镜像，而不让用户自由升级驱动？
3. 当您将数据从OSS传输到云服务器时，如何确保传输安全性和速度？
4. 如果云服务器网络差，服务商可能从哪些层面优化（如网络拓扑或协议）？
5. 在选择GPU实例时，除了显存大小，还有哪些因素（如GPU架构、虚拟化类型）会影响大模型训练效率？

请回答这些问题，我将根据您的回答指出漏洞并纠正。



### 

1. 云服务器中，虚拟机和容器在资源隔离和性能开销上有何主要区别？
   1. 虚拟机自己拥有完整的操作系统
   2. 容器则是共用操作系统，在上面添加一层层修改。
   3. 只不过，对于下层的修改是copy on write，保证下层不变。
   4. 性能上，虚拟机的开销更大一些，容器可以很方便多开，但是虚拟机每一个负担都很重。
   5. 资源隔离上，二者都很好，只不过，容器数据存储的持久化需要映射机制，而虚拟机则是已经包含了存储的空间。
   6. 所以，我很好奇，云服务器的厂商是如何平衡二者的。
   7. 之所以我会误会，因为我看到镜像和实例，就容易联想到Docker的image 和 进程，才意识到，其实虚拟机也有镜像这个说法。
2. 为什么服务商提供不同CUDA版本的镜像，而不让用户自由升级驱动？
   1. 兼容性和稳定性。
3. 当您将数据从OSS传输到云服务器时，如何确保传输安全性和速度？
   1. 这个不是我负责的时候，应该是提供商负责的事情。
4. 如果云服务器网络差，服务商可能从哪些层面优化（如网络拓扑或协议）？
   1. 在网络拓扑上，可以增加节点，协议上，可以换成UTP吗？这个不太清楚。
5. 在选择GPU实例时，除了显存大小，还有哪些因素（如GPU架构、虚拟化类型）会影响大模型训练效率？
   1. 这也是我的问题，我每一次购买的时候，看到非常多的型号的GPU，会犯选择性困难症。



感谢您的回答，这让我能精准地定位理解上的细微偏差。以下是基于您回答的分析和纠正：

### 对您答案的漏洞分析与纠正

**1. 关于虚拟机与容器**
您的理解基本正确，但可以更精确：
- **存储**：您提到“虚拟机已经包含了存储的空间”，这不完全准确。云服务器的虚拟机根磁盘通常也基于**云硬盘**，其持久化机制与容器挂载云硬盘在本质上类似，都是网络存储的映射。核心区别在于，虚拟机感知不到映射层，而容器需要显式声明卷挂载。
- **厂商的平衡**：主流云厂商的基础计算产品（ECS/EC2）几乎都是基于**虚拟机**，因为它提供了最强的安全隔离（不同用户的虚拟机运行在不同内核上）。**容器**（如Docker）是运行在虚拟机内部的应用交付技术。厂商通过提供“容器服务”（如Kubernetes集群）来管理底层虚拟机集群，从而平衡二者：虚拟机负责硬件与内核隔离，容器负责应用快速部署。

**2. 关于CUDA镜像与驱动**
“兼容性和稳定性”是正确的，但需要**深化**：
- **核心限制是驱动**：用户**不能**自由升级的是**GPU驱动**，而非CUDA Toolkit。驱动是内核级软件，与服务器内核版本深度绑定，由服务商统一测试和预装。强行升级可能导致整个系统不稳定或GPU无法识别。
- **灵活层级**：服务商通过提供不同版本的**系统镜像**（如Ubuntu 20.04 + Driver 470 + CUDA 11.4）来封装经过测试的驱动和基础CUDA环境。用户**可以在**此基础上，在容器或虚拟环境中，自行安装更高或更低版本的CUDA Toolkit和cuDNN，因为它们是用户态的库。

**3. 关于OSS数据传输的安全与速度**
您说“由提供商负责”是一个常见的误解。实际上，这是一个**责任共担模型**：
- **速度**：服务商负责提供高速内网通道（如OSS与ECS在同一地域时，走内网免费且高速）。但**用户需要确保**自己创建的实例和OSS桶在**同一地域**，并使用内网Endpoint地址，否则会自动走低速公网。
- **安全**：服务商负责底层基础设施安全。但**用户需要负责**：1）使用HTTPS协议传输；2）管理好Access Key（使用子账户或临时令牌），避免泄露；3）在OSS侧设置正确的Bucket Policy（读写权限）。

**4. 关于网络优化**
“增加节点”方向正确，“UTP”应为笔误（可能是UDP或TCP优化？）。服务商的优化是分层的：
- **物理与链路层**：接入更多运营商线路、购买更高带宽、建立互联网交换中心。
- **网络层**：通过BGP协议优化路由，让数据走更优路径。
- **传输/应用层**：部署TCP加速软件（如Google BBR）、为特定协议（如git）设置代理缓存、与核心内容提供商（如GitHub, HuggingFace）建立专线或合作。

**5. 关于GPU选择**
除了显存，关键因素包括：
- **GPU架构**：如NVIDIA的Ampere（A100/A10）、Ada Lovelace（4090/L40）或Hopper（H100）。新架构有更好的算力（如FP16/TF32性能）和能效。
- **虚拟化类型**：分为**直通**（整张物理卡独占，性能无损）和**虚拟化**（如vGPU，一张物理卡切分给多用户，有管理开销和性能损耗）。
- **GPU间互联带宽**：对于多卡训练，NVLink或PCIe版本和通道数决定了卡间通信速度，是影响扩展效率的关键。
- **显存带宽**：决定了从显存读写数据的速度，影响计算单元利用率。
- **配套CPU与内存**：避免CPU或内存成为瓶颈，需要平衡配置。

**总结**：您的思考方向完全正确，主要是一些**实践细节和权责划分**需要更清晰。云计算的复杂性正在于这些层层叠加的抽象和共担的责任模型。理解这些，您就能更主动地掌控云端环境，而不仅是“使用”它。



# 公网暴露服务[¶](https://gpushare.com/docs/best_practices/custom/#_1)

如何把自己的项目或者服务暴露到公网进行访问

自定义服务是平台提供的一个给用户暴露服务到公网访问的功能，可以暴露任何端口到公网进行访问，在启动需暴露服务时指定**`8080`**端口以及监听地址为 `0.0.0.0` 即可。

如下示例简单写了一个 Web 项目页面，然后暴露到公网访问的流程，让大家作为参考示范。

1.首先登录Linux实例。

登录实例过程可以参考：**[登录Linux实例](https://gpushare.com/docs/getting-started/login_instance/)** 相关文章。

2.示例项目代码如下

查看写的代码项目文件

```
(base) root@I139eda00f900b01e56:~# cat main.py 
```



代码具体内容如下

```
#!/usr/local/miniconda3/bin/python3
from http.server import SimpleHTTPRequestHandler, HTTPServer

host = '0.0.0.0' #定义web项目的监听地址，必须写为0.0.0.0，否则无法将公网正确转发到该项目中，坚决不能写为127.0.0.1，否则无法转发到实例对应的服务中
port = 8080      #定义web项目的监听端口，必须写为8080，否则无法将公网正确转发到该项目中

class MyRequestHandler(SimpleHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/ping':
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            response = f'PONG BY {host}:{port}'
            self.wfile.write(response.encode('utf-8'))
        else:
            self.send_response(200)
            self.send_header('Content-type', 'text/plain')
            self.end_headers()
            response = 'HELLO, GPUSHARE!'
            self.wfile.write(response.encode('utf-8'))

if __name__ == '__main__':
    server_address = (host, port)
    httpd = HTTPServer(server_address, MyRequestHandler)
    print(f'Starting server on {host}:{port}')
    httpd.serve_forever()
```



提示

如果是您自己写的项目，需要来定义监听地址和监听端口；如果是github或他人的项目，则需要寻找相应项目监听端口以及监听IP地址的代码文件或配置文件进行修改，部分项目可以通过启动项目时传入参数进行指定，例如 `python main.py --port 8080 --listen 0.0.0.0`。

3.启动项目

```
python main.py
```



4.验证项目是否启动成功 然后使用`ss -anplt | grep 8080`命令验证项目端口是否监听，如有监听则会返回如下面代码块中类似，否则无返回，就需要去查看项目启动日志是否出现问题。

```
ss -anplt | grep 8080
LISTEN    0         5                  0.0.0.0:8080             0.0.0.0:*        users:(("python",pid=16203,fd=3)) 
```



后台启动

上述默认为前台启动的web服务，可以通过**[tmux](https://gpushare.com/docs/best_practices/tmux/)**命令托管或通过nohup等命令来将visdom放置后台启动，放置后台启动不用担心终端断开而导致visdom服务中断问题。

5.通过公网访问项目 通过控制台**[我的实例](https://gpushare.com/center/hire)**中的【自定义服务】来访问您所需要暴露到公网的项目。

[![img](https://gpucloud-static-public-prod.gpushare.com/docs/image/best_practices/custom/custom_01.png)](https://gpucloud-static-public-prod.gpushare.com/docs/image/best_practices/custom/custom_01.png)